#!/bin/bash

# Backend for attention computation
# Available options:
# - "TORCH_SDPA": use torch.nn.MultiheadAttention
# - "FLASH_ATTN": use FlashAttention
# - "XFORMERS": use XFormers
# - "ROCM_FLASH": use ROCmFlashAttention
# - "FLASHINFER": use flashinfer
# - "FLASHMLA": use FlashMLA

# âœ… ä½œç”¨ï¼šæ‰€æœ‰ä½¿ç”¨ OpenMP çš„ CPU ç®—å­ï¼ˆMKL / OpenBLAS / oneDNN / éƒ¨åˆ† C++ æ‰©å±•ï¼‰æœ€å¤šåªå¼€ 1 ä¸ªçº¿ç¨‹
# ğŸ¯ å¸¸è§ç›®çš„ï¼šé˜²æ­¢å¤šè¿›ç¨‹åœºæ™¯ CPU çº¿ç¨‹çˆ†ç‚¸ï¼ˆæ¯ä¸ª worker å¼€å‡ åçº¿ç¨‹ â†’ ç›´æ¥æŠŠ CPU æ‰“æ»¡ï¼‰
export OMP_NUM_THREADS=1
export VLLM_ATTENTION_BACKEND=FLASH_ATTN
export VLLM_USE_FLASHINFER_SAMPLER=0  # é‡‡æ ·ç”¨ä¸ç”¨ FlashInfer


MODEL_PATH="Qwen2.5-VL-72B-Instruct"
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
python -m vllm.entrypoints.openai.api_server \
  --model "$MODEL_PATH" \
  --host 0.0.0.0 \
  --port 8000 \
  --trust-remote-code \
  --max-model-len 4096 \
  --tensor-parallel-size 8 \
  --gpu-memory-utilization 0.75 \
  --dtype bfloat16 \
  --max-num-seqs 32 \
  --disable-log-requests \
  --served-model-name qwen2.5-vl-72b-instruct \
