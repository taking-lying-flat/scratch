## 1. word2vec 是做什么的？

word2vec 的目标，是把每个词变成一个连续向量，让“出现在相似上下文里的词”在空间中靠得很近。这样一来，原本只是离散编号的词，就拥有了可以度量相似度、做加减运算的几何结构。Mikolov 等人提出的模型结构非常浅，却能在海量语料上高效训练，并学出能做类比推理的词向量，比如国家与首都、单数与复数之间的关系都能在向量空间里体现为类似的方向。:contentReference[oaicite:0]{index=0}  

## 2. Skip-gram 的直觉

在建模方式上，word2vec 主打 Skip-gram 结构：给定一句话中的一个中心词，让模型去预测它附近的上下文词。可以把它想成一个不断玩“看中间猜旁边”的游戏：每次拿一个词出来，让模型猜它左边和右边会出现哪些词。只要在大规模语料上反复做这个任务，每个词的向量就会逐渐学到自己典型的搭配环境。相比早期复杂的神经语言模型，这个结构只依赖查表和简单的向量运算，没有深层网络，因此在大词表上也能保持惊人的训练速度。:contentReference[oaicite:1]{index=1}  

## 3. 让大词表也训得动的技巧

为了在几十万甚至上百万的词汇下仍然可行，word2vec 提出了几项关键工程技巧。首先是用近似方法替代昂贵的完整输出层，例如负采样：不再对整张词表建模，而是把“真实的词–上下文配对”和“随机凑出的错误配对”混在一起做二分类，只需为每个正样本挑出少量负样本即可，大幅降低计算量。其次，对极高频但信息量很低的功能词（比如英文里的 the、of）采用按概率丢弃的策略，让训练更关注有内容的词，并给低频词更多更新机会。经验上，他们还发现把词频做一次 3/4 次方再归一化，用来采样负例，会比直接按词频更稳定、更有效。:contentReference[oaicite:2]{index=2}  

## 4. 从“词”扩展到“短语”

这篇工作没有把目标只停留在单词上，而是进一步构造了短语向量。作者通过简单的统计指标，从语料中挑出那些“经常一起出现、但单独出现又不算特别多”的词组，把它们合并成新的 token，例如 New\_York\_Times、machine\_learning 等，然后直接把这些短语当作普通词，和其他单词一起训练 Skip-gram 模型。实验表明，这样得到的短语向量依然具有良好的语义结构，可以和词向量一起参与类比与组合运算，从而更好地刻画真实语言中丰富的搭配关系。:contentReference[oaicite:3]{index=3}  

## 5. 影响与意义

总体来看，word2vec 的贡献，不在于提出一个多么“深”的网络，而在于用极简结构加上一系列巧妙的训练技巧，让大规模分布式词表示第一次变得既易算又好用。它展示了一个重要事实：只要有足够多的文本和合适的训练目标，就能在向量空间里自动涌现出大量语言规律，这些规律可以用来做相似度计算、类比推理和语义组合。随着训练代码和评测集的开源，word2vec 很快成为 NLP 社区的基础工具，也为后续的各种词向量方法以及如今的大型语言模型奠定了实践和概念上的重要基础。:contentReference[oaicite:4]{index=4}  
